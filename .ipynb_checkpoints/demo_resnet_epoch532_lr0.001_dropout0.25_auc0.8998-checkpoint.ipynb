{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Run ADBench \n",
    "- Here we provide a demo for testing AD algorithms on the datasets proposed in ADBench.\n",
    "- Feel free to evaluate any customized algorithm in ADBench.\n",
    "- For reproducing the complete experiment results in ADBench, please run the code in the run.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\autosoftware\\anconda\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from glob import glob\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载训练集的pkl文件，划分训练集与验证集\n",
    "训练集的label存放在pkl里面，可以通过它并区分正常片段和异常片段\n",
    "注意需要输入训练集对应的路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████▏                                                            | 5664/28389 [00:24<01:17, 294.25it/s]"
     ]
    }
   ],
   "source": [
    "ind_pkl_files = []#存放标签为0的文件\n",
    "ood_pkl_files = []#存放标签为1的文件\n",
    "data_path='Train'#存放数据的路径\n",
    "pkl_files = glob(data_path+'/*.pkl')\n",
    "for each_path in tqdm(pkl_files):\n",
    "    pic = open(each_path,'rb')\n",
    "    this_pkl_file= pickle.load(pic)#下载pkl文件\n",
    "    if this_pkl_file[1]['label'] == '00':\n",
    "        ind_pkl_files.append(each_path)\n",
    "    else:\n",
    "        ood_pkl_files.append(each_path)\n",
    "\n",
    "random.seed(0)\n",
    "#排序并打乱存放车辆序号的集合\n",
    "random.shuffle(ind_pkl_files)\n",
    "random.shuffle(ood_pkl_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ind_pkl_files))\n",
    "print(len(ood_pkl_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pkl_files=[]\n",
    "for j in range(len(ind_pkl_files)//4,len(ind_pkl_files)):\n",
    "#for j in range(len(ind_pkl_files)):\n",
    "    train_pkl_files.append(ind_pkl_files[j])\n",
    "#for i in range(len(ind_pkl_files)//4):\n",
    "#    train_pkl_files.append(ind_pkl_files[i])\n",
    "for j in range(len(ood_pkl_files)):\n",
    "    train_pkl_files.append(ood_pkl_files[j])\n",
    "print(len(train_pkl_files))\n",
    "test_pkl_files=[]\n",
    "#for j in range(len(ind_pkl_files)//4,len(ind_pkl_files)):\n",
    " #   test_pkl_files.append(ind_pkl_files[j])\n",
    "for i in range(len(ind_pkl_files)//4):\n",
    "    test_pkl_files.append(ind_pkl_files[i])\n",
    "#for item in ood_pkl_files:\n",
    "#    test_pkl_files.append(item)\n",
    "\n",
    "for j in range(len(ood_pkl_files)//4):\n",
    "    test_pkl_files.append(ood_pkl_files[j])\n",
    "print(len(test_pkl_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义加载函数，并对数据进行正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  load_data(pkl_list,label=True):\n",
    "    '''\n",
    "    输入pkl的列表，进行文件加载\n",
    "    label=True用来加载训练集\n",
    "    label=False用来加载真正的测试集，真正的测试集无标签\n",
    "    '''\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "\n",
    "    for  each_pkl in pkl_list:\n",
    "        pic = open(each_pkl,'rb')\n",
    "        item= pickle.load(pic)#下载pkl文件\n",
    "        # 此处选取的是每个滑窗的最后一条数据，仅供参考，可以选择其他的方法，比如均值或者其他处理时序数据的网络\n",
    "        # 此处选取了前7个特征，可以需求选取特征数量\n",
    "        feature = item[0][:,0:7][-1]\n",
    "        #feature = item[0][:,0:7][-1]\n",
    "        #feature = item[0][:,0:7].mean(axis=0)\n",
    "        #feature = np.append(item[0][:,0:7][-1],(item[0][:,3][-1] - item[0][:,4][-1])) #加max_single_volt - min_single_volt 一列为特征\n",
    "        feature=np.append(feature,item[1][\"mileage\"])\n",
    "        X.append(feature)\n",
    "        if label:\n",
    "            y.append(int(item[1]['label'][0]))\n",
    "    X = np.vstack(X)\n",
    "    if label:\n",
    "        y = np.vstack(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,y_train=load_data(train_pkl_files)\n",
    "X_test,y_test=load_data(test_pkl_files)\n",
    "_mean = np.mean(X_train, axis=0)\n",
    "_std = np.std(X_train, axis=0)\n",
    "X_train = (X_train - _mean) / (_std + 1e-4)\n",
    "X_test = (X_test - _mean) / (_std + 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils import shuffle\n",
    "# 进行随机打乱，这里random_state指定为固定值，则打乱结果相同\n",
    "X_train,y_train = shuffle(X_train,y_train,random_state=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path3='Test_A'\n",
    "test1_files = glob(data_path3+'/*.pkl')\n",
    "X_val,_=load_data(test1_files,label=False)\n",
    "_mean = np.mean(X_val, axis=0)\n",
    "_std = np.std(X_val, axis=0)\n",
    "X_val = (X_val - _mean) / (_std + 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# import zero\n",
    "import delu # the new version of zero package\n",
    "import rtdl\n",
    "import scipy\n",
    "import platform\n",
    "from myutils import Utils\n",
    "utils = Utils() # utils function\n",
    "\n",
    "\n",
    "class FTTransformer():\n",
    "    '''\n",
    "    The original code: https://yura52.github.io/rtdl/stable/index.html\n",
    "    The original paper: \"Revisiting Deep Learning Models for Tabular Data\", NIPS 2019\n",
    "    '''\n",
    "    def __init__(self, seed:int, model_name:str, n_epochs=100, batch_size=64):\n",
    "\n",
    "        self.seed = seed\n",
    "        self.model_name = model_name\n",
    "        self.utils = Utils()\n",
    "\n",
    "        # device\n",
    "        if model_name == 'FTTransformer':\n",
    "            self.device = self.utils.get_device(gpu_specific=True)\n",
    "        else:\n",
    "            self.device = self.utils.get_device(gpu_specific=False)\n",
    "\n",
    "        # Docs: https://yura52.github.io/zero/0.0.4/reference/api/zero.improve_reproducibility.html\n",
    "        # zero.improve_reproducibility(seed=self.seed)\n",
    "        delu.improve_reproducibility(base_seed=int(self.seed))\n",
    "\n",
    "        # hyper-parameter\n",
    "        self.n_epochs = n_epochs # default is 1000\n",
    "        self.batch_size = batch_size # default is 256\n",
    "\n",
    "    def apply_model(self, x_num, x_cat=None):\n",
    "        if isinstance(self.model, rtdl.FTTransformer):\n",
    "            return self.model(x_num, x_cat)\n",
    "        elif isinstance(self.model, (rtdl.MLP, rtdl.ResNet)):\n",
    "            assert x_cat is None\n",
    "            return self.model(x_num)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f'Looks like you are using a custom model: {type(self.model)}.'\n",
    "                ' Then you have to implement this branch first.'\n",
    "            )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, X, y=None):\n",
    "        self.model.eval()\n",
    "        score = []\n",
    "        # for batch in delu.iter_batches(X[part], 1024):\n",
    "        for batch in delu.iter_batches(X, self.batch_size):\n",
    "            score.append(self.apply_model(batch))\n",
    "        score = torch.cat(score).squeeze(1).cpu().numpy()\n",
    "        score = scipy.special.expit(score)\n",
    "\n",
    "        # calculate the metric\n",
    "        if y is not None:\n",
    "            target = y.cpu().numpy()\n",
    "            metric = self.utils.metric(y_true=target, y_score=score)\n",
    "        else:\n",
    "            metric = {'aucroc': None, 'aucpr': None}\n",
    "\n",
    "        return score, metric['aucpr']\n",
    "\n",
    "    def fit(self, X_train, y_train, ratio=None,X_test=X_test,y_test=y_test):\n",
    "        # set seed\n",
    "        self.utils.set_seed(self.seed)\n",
    "       \n",
    "        #X_train, X_test_val, y_train, y_test_val = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "        # training set is used as the validation set in the anomaly detection task\n",
    "        X = {'train': torch.from_numpy(X_train).float().to(self.device),\n",
    "             'val': torch.from_numpy(X_train).float().to(self.device)}\n",
    "\n",
    "        y = {'train': torch.from_numpy(y_train).float().to(self.device),\n",
    "             'val': torch.from_numpy(y_train).float().to(self.device)}\n",
    "        \n",
    "       #training set is used as the validation set in the anomaly detection task\n",
    "#         X = {'train': torch.from_numpy(X_train).float().to(self.device),\n",
    "#             'val': torch.from_numpy(X_test_val).float().to(self.device)}\n",
    "\n",
    "#         y = {'train': torch.from_numpy(y_train).float().to(self.device),\n",
    "#             'val': torch.from_numpy(y_test_val).float().to(self.device)}\n",
    "\n",
    "        task_type = 'binclass'\n",
    "        n_classes = None\n",
    "        d_out = n_classes or 1\n",
    "\n",
    "\n",
    "        if self.model_name == 'ResNet':\n",
    "            self.model = rtdl.ResNet.make_baseline(\n",
    "                d_in=X_train.shape[1],\n",
    "                d_main=128,\n",
    "                d_hidden=256,\n",
    "                dropout_first=0.25,\n",
    "                dropout_second=0,\n",
    "                n_blocks=2,\n",
    "                d_out=d_out,\n",
    "            )\n",
    "            lr = 0.001\n",
    "            weight_decay = 0.0\n",
    "        \n",
    "        elif self.model_name == 'MLP':\n",
    "            self.model = rtdl.MLP.make_baseline(\n",
    "            d_in=X_train.shape[1],\n",
    "            d_layers= [128, 256, 128],\n",
    "            dropout=0.25,\n",
    "            d_out=d_out,\n",
    "            )\n",
    "            lr = 0.001\n",
    "            weight_decay = 0.0\n",
    "\n",
    "        elif self.model_name == 'FTTransformer':\n",
    "            self.model = rtdl.FTTransformer.make_default(\n",
    "                n_num_features=X_train.shape[1],\n",
    "                cat_cardinalities=None,\n",
    "                last_layer_query_idx=[-1],  # it makes the model faster and does NOT affect its output\n",
    "                d_out=d_out,\n",
    "            )\n",
    "            \n",
    "        elif self.model_name == 'FTTransformer_baseline':\n",
    "            self.model = rtdl.FTTransformer.make_baseline(\n",
    "                n_num_features=X_train.shape[1],\n",
    "                cat_cardinalities=None,\n",
    "                d_token=X_train.shape[1],\n",
    "                n_blocks=2,\n",
    "                attention_dropout=0.2,\n",
    "                ffn_d_hidden=6,\n",
    "                ffn_dropout=0.2,\n",
    "                residual_dropout=0.0,\n",
    "                d_out=d_out,\n",
    "            ) \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        optimizer = (\n",
    "            self.model.make_default_optimizer()\n",
    "            if isinstance(self.model, rtdl.FTTransformer)\n",
    "            else torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        )\n",
    "        loss_fn = (\n",
    "            F.binary_cross_entropy_with_logits\n",
    "            if task_type == 'binclass'\n",
    "            else F.cross_entropy\n",
    "            if task_type == 'multiclass'\n",
    "            else F.mse_loss\n",
    "        )\n",
    "\n",
    "        # Create a dataloader for batches of indices\n",
    "        # Docs: https://yura52.github.io/zero/reference/api/zero.data.IndexLoader.html\n",
    "        train_loader = delu.data.IndexLoader(len(X['train']), self.batch_size, device=self.device)\n",
    "\n",
    "        # Create a progress tracker for early stopping\n",
    "        # Docs: https://yura52.github.io/zero/reference/api/zero.ProgressTracker.html\n",
    "        progress = delu.ProgressTracker(patience=100)\n",
    "\n",
    "        # training\n",
    "        # report_frequency = len(X['train']) // self.batch_size // 5\n",
    "\n",
    "        for epoch in range(1, self.n_epochs + 1):\n",
    "            loss_tmp = []\n",
    "            for iteration, batch_idx in enumerate(train_loader):\n",
    "                self.model.train()\n",
    "                optimizer.zero_grad()\n",
    "                x_batch = X['train'][batch_idx]\n",
    "                y_batch = y['train'][batch_idx]\n",
    "                loss = loss_fn(self.apply_model(x_batch).squeeze(1), y_batch)\n",
    "                loss_tmp.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # if iteration % report_frequency == 0:\n",
    "                #     print(f'(epoch) {epoch} (batch) {iteration} (loss) {loss.item():.4f}')\n",
    "\n",
    "            loss_.append(sum(loss_tmp)/len(loss_tmp))\n",
    "            _, val_metric = self.evaluate(X=X['val'], y=y['val'])\n",
    "            print(f'Epoch {epoch:03d} | Validation metric: {val_metric:.4f}', end='')\n",
    "            progress.update((-1 if task_type == 'regression' else 1) * val_metric)\n",
    "            if progress.success:\n",
    "                print(' <<< BEST VALIDATION EPOCH', end='')\n",
    "            print()\n",
    "            # 验证\n",
    "            # output predicted anomaly score on testing set\n",
    "            score = self.predict_score(X_test)\n",
    "            # evaluation\n",
    "            result = utils.metric(y_true=y_test, y_score=score)\n",
    "            aucroc.append(result['aucroc'])\n",
    "            aucpr.append(result['aucpr'])\n",
    "            if progress.fail:\n",
    "                break\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_score(self, X):\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "        score, _ = self.evaluate(X=X, y=None)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "clf=FTTransformer(seed,\"ResNet\",n_epochs=532,batch_size=256)\n",
    "aucroc = []\n",
    "aucpr = []\n",
    "loss_ = []\n",
    "clf = clf.fit(X_train=X_train, y_train=y_train.squeeze(1),X_test=X_test,y_test=y_test)\n",
    "import platform\n",
    "y_val_scores = clf.predict_score(X_val)   #返回未知数据上的异常值 (分值越大越异常) # outlier scores\n",
    "#记录文件名和对应的异常得分\n",
    "predict_result={}\n",
    "for i in tqdm(range(len(test1_files))):\n",
    "    file=test1_files[i]\n",
    "    #如果是window系统：\n",
    "    if platform.system().lower() == 'windows':\n",
    "        name=file.split('\\\\')[-1]\n",
    "    #如果是linux系统\n",
    "    elif platform.system().lower() == 'linux':\n",
    "        name=file.split('/')[-1]\n",
    "    predict_result[name]=y_val_scores[i]\n",
    "predict_score=pd.DataFrame(list(predict_result.items()),columns=['file_name','score'])#列名必须为这俩个\n",
    "predict_score.to_csv(f'submision.csv',index = False) #保存为比赛要求的csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=((10,8)))\n",
    "plt.subplot(221)\n",
    "plt.plot(aucroc,label=\"aucroc\")\n",
    "plt.plot(aucpr,label=\"aucpr\")\n",
    "plt.legend()\n",
    "plt.subplot(222)\n",
    "plt.plot(loss_,label=\"loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print(aucroc.index(max(aucroc)))\n",
    "print(aucpr.index(max(aucpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thop import clever_format\n",
    "from thop import profile\n",
    "input = torch.randn(256, 8)\n",
    "flops, params =profile(clf.model, inputs=(input,))\n",
    "flops, params = clever_format([flops, params], \"%.3f\")\n",
    "print(flops)\n",
    "print(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
