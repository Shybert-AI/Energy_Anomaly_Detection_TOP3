{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Run ADBench \n",
    "- Here we provide a demo for testing AD algorithms on the datasets proposed in ADBench.\n",
    "- Feel free to evaluate any customized algorithm in ADBench.\n",
    "- For reproducing the complete experiment results in ADBench, please run the code in the run.py file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\autosoftware\\anconda\\lib\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import argparse\n",
    "from sklearn.utils import shuffle\n",
    "#import sklearn.metrics\n",
    "#import sklearn.preprocessing\n",
    "import torch\n",
    "#import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import delu\n",
    "import rtdl\n",
    "import scipy\n",
    "import platform\n",
    "from matplotlib import pyplot as plt\n",
    "from myutils import Utils\n",
    "import optuna\n",
    "utils = Utils() # utils function\n",
    "\n",
    "\n",
    "\n",
    "def  load_data(pkl_list,label=True):\n",
    "    '''\n",
    "    输入pkl的列表，进行文件加载\n",
    "    label=True用来加载训练集\n",
    "    label=False用来加载真正的测试集，真正的测试集无标签\n",
    "    '''\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "\n",
    "    for  each_pkl in pkl_list:\n",
    "        pic = open(each_pkl,'rb')\n",
    "        item= pickle.load(pic)#下载pkl文件\n",
    "        # 此处选取的是每个滑窗的最后一条数据，仅供参考，可以选择其他的方法，比如均值或者其他处理时序数据的网络\n",
    "        # 此处选取了前7个特征，可以需求选取特征数量\n",
    "        feature = item[0][:,0:7][-1]\n",
    "        #feature = item[0][:,0:7][-1]\n",
    "        #feature = item[0][:,0:7].mean(axis=0)\n",
    "        #feature = np.append(item[0][:,0:7][-1],(item[0][:,3][-1] - item[0][:,4][-1])) #加max_single_volt - min_single_volt 一列为特征\n",
    "        feature=np.append(feature,item[1][\"mileage\"])\n",
    "        X.append(feature)\n",
    "        if label:\n",
    "            y.append(int(item[1]['label'][0]))\n",
    "    X = np.vstack(X)\n",
    "    if label:\n",
    "        y = np.vstack(y)\n",
    "    return X, y\n",
    "    \n",
    "def normalization(data): \n",
    "    \"\"\"\n",
    "    归一化数据\n",
    "    \"\"\"\n",
    "    _mean = np.mean(data, axis=0)\n",
    "    _std = np.std(data, axis=0)\n",
    "    data = (data - _mean) / (_std + 1e-4)\n",
    "    return data\n",
    "\n",
    "\n",
    "class FTTransformer():\n",
    "    '''\n",
    "    The original code: https://yura52.github.io/rtdl/stable/index.html\n",
    "    The original paper: \"Revisiting Deep Learning Models for Tabular Data\", NIPS 2019\n",
    "    '''\n",
    "    def __init__(self, seed:int, model_name:str, n_epochs=100, batch_size=64):\n",
    "\n",
    "        self.seed = seed\n",
    "        self.model_name = model_name\n",
    "        self.utils = Utils()\n",
    "\n",
    "        # device\n",
    "        if model_name == 'FTTransformer':\n",
    "            self.device = self.utils.get_device(gpu_specific=True)\n",
    "        else:\n",
    "            self.device = self.utils.get_device(gpu_specific=False)\n",
    "\n",
    "        # Docs: https://yura52.github.io/zero/0.0.4/reference/api/zero.improve_reproducibility.html\n",
    "        # zero.improve_reproducibility(seed=self.seed)\n",
    "        delu.improve_reproducibility(base_seed=int(self.seed))\n",
    "\n",
    "        # hyper-parameter\n",
    "        self.n_epochs = n_epochs # default is 1000\n",
    "        self.batch_size = batch_size # default is 256\n",
    "\n",
    "    def apply_model(self, x_num, x_cat=None):\n",
    "        if isinstance(self.model, rtdl.FTTransformer):\n",
    "            return self.model(x_num, x_cat)\n",
    "        elif isinstance(self.model, (rtdl.MLP, rtdl.ResNet)):\n",
    "            assert x_cat is None\n",
    "            return self.model(x_num)\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                f'Looks like you are using a custom model: {type(self.model)}.'\n",
    "                ' Then you have to implement this branch first.'\n",
    "            )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, X, y=None):\n",
    "        self.model.eval()\n",
    "        score = []\n",
    "        # for batch in delu.iter_batches(X[part], 1024):\n",
    "        for batch in delu.iter_batches(X, self.batch_size):\n",
    "            score.append(self.apply_model(batch))\n",
    "        score = torch.cat(score).squeeze(1).cpu().numpy()\n",
    "        score = scipy.special.expit(score)\n",
    "\n",
    "        # calculate the metric\n",
    "        if y is not None:\n",
    "            target = y.cpu().numpy()\n",
    "            metric = self.utils.metric(y_true=target, y_score=score)\n",
    "        else:\n",
    "            metric = {'aucroc': None, 'aucpr': None}\n",
    "\n",
    "        return score, metric['aucpr']\n",
    "\n",
    "    def fit(self, X_train, y_train, ratio=None,X_test=None,y_test=None,params=None):\n",
    "        # set seed\n",
    "        self.utils.set_seed(self.seed)\n",
    "       \n",
    "        #X_train, X_test_val, y_train, y_test_val = train_test_split(X_train, y_train, test_size=0.33, random_state=42)\n",
    "        # training set is used as the validation set in the anomaly detection task\n",
    "        X = {'train': torch.from_numpy(X_train).float().to(self.device),\n",
    "             'val': torch.from_numpy(X_train).float().to(self.device)}\n",
    "\n",
    "        y = {'train': torch.from_numpy(y_train).float().to(self.device),\n",
    "             'val': torch.from_numpy(y_train).float().to(self.device)}\n",
    "        \n",
    "       #training set is used as the validation set in the anomaly detection task\n",
    "#         X = {'train': torch.from_numpy(X_train).float().to(self.device),\n",
    "#             'val': torch.from_numpy(X_test_val).float().to(self.device)}\n",
    "\n",
    "#         y = {'train': torch.from_numpy(y_train).float().to(self.device),\n",
    "#             'val': torch.from_numpy(y_test_val).float().to(self.device)}\n",
    "\n",
    "        task_type = 'binclass'\n",
    "        n_classes = None\n",
    "        d_out = n_classes or 1\n",
    "\n",
    "        if self.model_name == 'ResNet':\n",
    "            self.model = rtdl.ResNet.make_baseline(\n",
    "                d_in=X_train.shape[1],\n",
    "                d_main=128,\n",
    "                d_hidden=256,\n",
    "                dropout_first=params['dropout_first'],\n",
    "                dropout_second=0.0,\n",
    "                n_blocks=params['n_blocks'],\n",
    "                d_out=d_out,\n",
    "            )\n",
    "            lr = params['learning_rate']\n",
    "            weight_decay = 0.0\n",
    "        \n",
    "        elif self.model_name == 'MLP':\n",
    "            self.model = rtdl.MLP.make_baseline(\n",
    "            d_in=X_train.shape[1],\n",
    "            d_layers= [128, 256, 128],\n",
    "            dropout=0.25,\n",
    "            d_out=d_out,\n",
    "            )\n",
    "            lr = 0.001\n",
    "            weight_decay = 0.0\n",
    "\n",
    "        elif self.model_name == 'FTTransformer':\n",
    "            self.model = rtdl.FTTransformer.make_default(\n",
    "                n_num_features=X_train.shape[1],\n",
    "                cat_cardinalities=None,\n",
    "                last_layer_query_idx=[-1],  # it makes the model faster and does NOT affect its output\n",
    "                d_out=d_out,\n",
    "            )\n",
    "            \n",
    "        elif self.model_name == 'FTTransformer_baseline':\n",
    "            self.model = rtdl.FTTransformer.make_baseline(\n",
    "                n_num_features=X_train.shape[1],\n",
    "                cat_cardinalities=None,\n",
    "                d_token=X_train.shape[1],\n",
    "                n_blocks=2,\n",
    "                attention_dropout=0.2,\n",
    "                ffn_d_hidden=6,\n",
    "                ffn_dropout=0.2,\n",
    "                residual_dropout=0.0,\n",
    "                d_out=d_out,\n",
    "            ) \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        optimizer = (\n",
    "            self.model.make_default_optimizer()\n",
    "            if isinstance(self.model, rtdl.FTTransformer)\n",
    "            else torch.optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        )\n",
    "        loss_fn = (\n",
    "            F.binary_cross_entropy_with_logits\n",
    "            if task_type == 'binclass'\n",
    "            else F.cross_entropy\n",
    "            if task_type == 'multiclass'\n",
    "            else F.mse_loss\n",
    "        )\n",
    "\n",
    "        # Create a dataloader for batches of indices\n",
    "        # Docs: https://yura52.github.io/zero/reference/api/zero.data.IndexLoader.html\n",
    "        train_loader = delu.data.IndexLoader(len(X['train']), self.batch_size, device=self.device)\n",
    "\n",
    "        # Create a progress tracker for early stopping\n",
    "        # Docs: https://yura52.github.io/zero/reference/api/zero.ProgressTracker.html\n",
    "        progress = delu.ProgressTracker(patience=100)\n",
    "\n",
    "        # training\n",
    "        # report_frequency = len(X['train']) // self.batch_size // 5\n",
    "        aucroc = []\n",
    "        aucpr = []\n",
    "        loss_ = []\n",
    "        for epoch in range(1, self.n_epochs + 1):\n",
    "            loss_tmp = []\n",
    "            for iteration, batch_idx in enumerate(train_loader):\n",
    "                self.model.train()\n",
    "                optimizer.zero_grad()\n",
    "                x_batch = X['train'][batch_idx]\n",
    "                y_batch = y['train'][batch_idx]\n",
    "                loss = loss_fn(self.apply_model(x_batch).squeeze(1), y_batch)\n",
    "                loss_tmp.append(loss.item())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # if iteration % report_frequency == 0:\n",
    "                #     print(f'(epoch) {epoch} (batch) {iteration} (loss) {loss.item():.4f}')\n",
    "\n",
    "            loss_.append(sum(loss_tmp)/len(loss_tmp))\n",
    "            _, val_metric = self.evaluate(X=X['val'], y=y['val'])\n",
    "            print(f'Epoch {epoch:03d} | Validation metric: {val_metric:.4f}', end='')\n",
    "            progress.update((-1 if task_type == 'regression' else 1) * val_metric)\n",
    "            if progress.success:\n",
    "                print(' <<< BEST VALIDATION EPOCH', end='')\n",
    "            print()\n",
    "            # 验证\n",
    "            # output predicted anomaly score on testing set\n",
    "            score = self.predict_score(X_test)\n",
    "            # evaluation\n",
    "            result = utils.metric(y_true=y_test, y_score=score)\n",
    "            aucroc.append(result['aucroc'])\n",
    "            aucpr.append(result['aucpr'])\n",
    "            if progress.fail:\n",
    "                break\n",
    "        return result['aucroc']\n",
    "        #return self\n",
    "\n",
    "    def predict_score(self, X):\n",
    "        X = torch.from_numpy(X).float().to(self.device)\n",
    "        score, _ = self.evaluate(X=X, y=None)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 28389/28389 [03:54<00:00, 120.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22456\n",
      "7096\n"
     ]
    }
   ],
   "source": [
    "data_path3 = \"Test_A\"\n",
    "epoch = 10\n",
    "batch_size = 256\n",
    "model_name = \"ResNet\"\n",
    "# 加载训练集的pkl文件，划分训练集与验证集\n",
    "ind_pkl_files = []#存放标签为0的文件\n",
    "ood_pkl_files = []#存放标签为1的文件\n",
    "data_path=\"Train\"#存放数据的路径\n",
    "pkl_files = glob(data_path+'/*.pkl')\n",
    "for each_path in tqdm(pkl_files):\n",
    "    pic = open(each_path,'rb')\n",
    "    this_pkl_file= pickle.load(pic)#下载pkl文件\n",
    "    if this_pkl_file[1]['label'] == '00':\n",
    "        ind_pkl_files.append(each_path)\n",
    "    else:\n",
    "        ood_pkl_files.append(each_path)\n",
    "\n",
    "random.seed(0)\n",
    "# 排序并打乱存放车辆序号的集合\n",
    "random.shuffle(ind_pkl_files)\n",
    "random.shuffle(ood_pkl_files)\n",
    "# 3/4的正样本和全部的负样本作为训练集，1/4的正样本和1/4的负样本作为训练集\n",
    "train_pkl_files = [ ind_pkl_files[j] for j in range(len(ind_pkl_files)//4,len(ind_pkl_files))] + [ ood_pkl_files[i] for i in range(len(ood_pkl_files))]\n",
    "test_pkl_files=[ind_pkl_files[i] for i in range(len(ind_pkl_files)//4)] + [ood_pkl_files[i] for i in range(len(ood_pkl_files)//4)]\n",
    "\n",
    "print(len(train_pkl_files))\n",
    "print(len(test_pkl_files))\n",
    "\n",
    "# 加载并归一化训练数据和验证数据\n",
    "X_train,y_train=load_data(train_pkl_files)\n",
    "# 进行随机打乱，这里random_state指定为固定值，则打乱结果相同\n",
    "X_train,y_train = shuffle(X_train,y_train,random_state=40)\n",
    "X_test,y_test=load_data(test_pkl_files)\n",
    "X_train = normalization(X_train)\n",
    "X_test = normalization(X_test)\n",
    "\n",
    "test1_files = glob(data_path3+'/*.pkl')\n",
    "X_val,_=load_data(test1_files,label=False)\n",
    "X_val = normalization(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    seed = 42\n",
    "    clf=FTTransformer(seed,model_name,n_epochs=10,batch_size=batch_size)\n",
    "\n",
    "    params = {\n",
    "              'learning_rate': trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True),\n",
    "              'dropout_first': trial.suggest_float('dropout_first', 0.1, 0.5),\n",
    "              'n_blocks': trial.suggest_int(\"n_blocks\", 1, 4),\n",
    "              }\n",
    "\n",
    "    accuracy = clf.fit(X_train=X_train, y_train=y_train.squeeze(1),X_test=X_test,y_test=y_test,params=params)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials=10)\n",
    "best_trial = study.best_trial\n",
    "print(best_trial.value)\n",
    "for key, value in best_trial.params.items():\n",
    "    print(\"{}: {}\".format(key, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
